# 代码执行

## 项目概述

执行逻辑：在此目录（`./`）执行`python code/run.py`或其他python文件

## 修改记录

活动记录：

| 事项         | 人   | 详细描述                                                    | 进度 |
| ------------ | ---- | ----------------------------------------------------------- | ---- |
| 比赛         | all  | 完成，最终召回率0.71778039，排名19/112                      | 完成 |
| zxw          |      |                                                             |      |
| 数据清洗     | zxw  | 对Markdown和Html的文本很快进行了清洗                        | 完成 |
| 数据收集     | zxw  | 收集了新数据，使用Hash和本地Cache降低时间开销               | 完成 |
| bzh          |      |                                                             |      |
| 代码重构     | bzh  | 将NoteBook重构为了项目化的代码                              | 完成 |
| 模型替换     | bzh  | Qwen7B的效果极差，本地微调在选择上不好用                    | 完成 |
| 模型微调     | bzh  | 对Qwen7B进行了微调，但是效果还是不好                        | 完成 |
| prompt优化   | bzh  | 对输出的选择、RAG的prompt均进行了修改，提升客观             | 完成 |
| 提问改写     | bzh  | 利用LLM填补缺失信息，以防问题本身造成查询困扰               | 完成 |
| 文本召回重排 | bzh  | 对原始问题和重写问题分别进行召回，得到相似度最高的num个问题 | 完成 |
| 回答风格改写 | bzh  | 为匹配接近答案的语言风格，在得到答案后由LLM进行一次改写概括 | 完成 |
| hj           |      |                                                             |      |
| 优化查询     | hj   | 修改了Embedding模型选择                                     | 完成 |

## 版本记录

答案的版本信息：

- v1.0：仅使用最基本的baseline
- v2.0：加入混合检索，引入了问题重新描述
- v2.5：拆分了RAG，进行了召回，优化了prompt
- v3.0：召回率来到0.69
  - 更新了数据集，但是还是没有清洗
  - 使用简化回答的prompt
- v3.1：召回率0.699
  - 更新了异名同义文档
  - 对召回进行了优化，对原始问题和改写问题同时进行查找，取召回率最高的前num个结果
  - 优化了问题改写的prompt
  - 修改prompt为尽量使用中文回答问题
- v3.2：召回率0.6985
  - 更新了数据源，使用了MR中的数据清洗
- v3.3：召回率0.6839
  - 进一步更新数据源，使用了MR中的数据清洗
  - 对一些问题使用了专门的数据（70、73、76）
- v4.0：召回率0.7036
  - 回退到v3.1
  - 增加了针对性数据集
- v4.1：召回率0.7124
  - 优化部分prompt，尝试让答案更简洁

# 赛题介绍

## 赛题任务

参赛者需要开发一个智能问答系统，能够准确回答关于TuGraph-DB的各类问题。问题主要分为以下四类：

1. **基于单个文档或代码段的问题**：这类问题旨在测试系统的基础检索能力和对单一信息源的理解能力。例如，"如何在TuGraph-DB中创建一个新的图实例？"这类问题通常可以在单个文档中找到直接答案。
2. **需要综合多个文档或代码段的问题**：这类问题考察系统的信息整合能力和跨文档推理能力。例如，"TuGraph-DB在处理大规模图数据时有哪些优化策略？"回答这个问题可能需要从多个文档中提取信息并进行综合。
3. **无法回答的问题**：这类问题测试系统识别信息边界的能力。例如，询问TuGraph-DB当前不支持的功能或文档中未提及的内容。系统应该能够准确识别这些情况并给出恰当的回应，如：暂不支持该功能。
4. **代码理解类任务**：这类问题考察系统对TuGraph-DB特定语法和代码的理解能力。包括解释GQL（Graph Query Language）语句。例如，“请解释以下GQL查询的功能：[具体GQL查询]”

## 赛题提示

1. 由于数据取自真实场景，因此问题可能是简略的、口语化的、不完整的，比如：“想将关系型数据库数据导入图库”里面缺少了“TuGraph-DB”，完整的问题应该为“如何将关系型数据库数据导入TuGraph-DB中？”。建议选手对该情形进行专门设计。
2. 本次比赛不提供训练语料，建议选手搜集TuGraph-DB相关公开外部语料，比如：
   - TuGraph-DB使用文档：详细介绍了TuGraph-DB的各项功能、使用方法、最佳实践等。
   - TuGraph-DB开源代码：包含TuGraph-DB的源代码，可用于理解底层实现和回答技术细节问题。
   - TuGraph微信公众号文章（ID：TuGraph）
3. 在大语言模型选择上，参赛选手还需要考虑模型占用显存大小，以及推理效率。
4. 建议使用RAG相关技术，以增强大语言模型TuGraph-DB相关知识。

## 数据说明

比赛只提供验证集，其中包含50条问答数据，每条数据包括三个字段：

- id：问题编号
- input_field：输入问题
- output_field：标准答案

测试数据集将只提供id和input_field，不提供output_field，总共包含450条问答数据。其中初赛测试数据为250条，复赛测试数据为200条。

对于训练数据，参赛者可以使用自构建数据或其他开源数据集。

## 提交要求

**测试集预测结果文件**：所有参赛队伍在初赛阶段需要提交一个JSONL格式的文件，文件命名为answer.jsonl，其中包含对测试集中每个问题的回答。每条数据应包含以下两个字段：

- id：对应的问题编号
- output_field：模型预测的答案

**项目源代码**：进入复赛的队伍需要提供问答系统的源代码、模型参数文件（需要上传到魔搭社区）、训练数据以及Prompts。此外，还需要包含一个详细介绍实现方案的README.md文件与一个能够运行后输出预测结果的run.sh脚本（请注意确保你的run.sh可执行）。每支队伍可提交2份代码、模型，最终取二者最优结果作为队伍的最终成绩。超出时间限制的提交，视为无效提交。
测试环境目录结构如下：

```
├── ground_truth.jsonl # 由主办方添加
├── evaluation.py # 由主办方添加
└── your_project
    ├── download.py
    ├── output
    │   └── answer.jsonl # 通过run.sh获得
    ├── run.sh
    ├── README.md
    └── training_data
```

`ground_truth.jsonl`为测试数据集的标准答案，`evaluation.py`为评测代码，由主办方进行添加。`your_project`文件夹是选手的项目文件夹。选手需提供进行模型下载的`download.py`脚本，和执行完整翻译流程的`run.sh`脚本，并将模型预测结果输出到output文件夹中，命名为`answer.jsonl`。此外，还需要在READEME.md中简要介绍方案，并将训练数据放入`training_data`中。在评测过程中，自动测试脚本会首先下载参赛队伍提交的项目文件夹，在执行`download.py`完成模型下载后执行run.sh脚本，最后读取`answer.jsonl`中的结果进行评分，流程示例如下：

```bash
cd ${YOUR_PROJECT}
python3 download.py
bash run.sh # 注意不要超过限制时间
cd ..
python3 evalutaion.py
```

## 评测方式

比赛测评分为初赛、复赛、决赛三个阶段。

**初赛阶段**：参赛者需要读取包含自然语言问题描述与对应数据库id的测试文件，使用大语言模型对问题进行回答，并将预测结果按照样例格式输出到命名为`answer.jsonl`的文件中并提交。初赛阶段在线评测程序将会实时反馈分数与排名，排名前10的选手将会进入复赛。

**复赛阶段**：不公开测试数据，参赛者按要求提交源代码及模型，平台自动评测。复赛期间每日每队最多可提交1次，评测结果在次日更新，各队伍仅可查看评估是否成功，不可查看具体分数。提交截止后，对各队伍提交的代码模型进行审核，并以所有提交记录中的最高成绩作为队伍最终复赛分数，前5名进入决赛。

**决赛阶段**：采用线上答辩形式进行，5支队伍依次展示作品思路，回答评委提出的问题，所有队伍结束路演答辩后，评委结合各队伍的模型线上客观效果排名、作品创新性及应用价值、答辩表现等进行打分，根据所有评委打分的和计算各队伍的最终决赛成绩，根据排名确定单赛题奖项。

## 评测标准

评测采用基于语义相似度的方法，具体来说：使用Sentence Transformer将模型生成的答案和标准答案转换为句子嵌入表示，然后计算这两个嵌入表示之间的余弦相似度（并归一化为[0,1]）作为分数。
