{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局集合，用于存储已访问过的 URL\n",
    "visited = set()\n",
    "# 用于存储成功访问的 URL\n",
    "successful_urls = []\n",
    "# 用于存储每个页面内容的哈希值，去重使用\n",
    "content_hashes = set()\n",
    "\n",
    "# 计算页面内容的哈希值，避免重复\n",
    "def get_content_hash(content):\n",
    "    return hashlib.md5(content.encode('utf-8')).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义递归函数，爬取每个页面\n",
    "def crawl(url, base_url):\n",
    "    # 如果已经访问过此URL，则跳过\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)  # 记录访问过的URL\n",
    "\n",
    "    # 请求页面内容\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # 检查请求是否成功\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return\n",
    "\n",
    "    # 计算页面内容的哈希值\n",
    "    content_hash = get_content_hash(response.text)\n",
    "\n",
    "    # 如果该页面内容的哈希值已经存在，说明内容重复，跳过该页面\n",
    "    if content_hash in content_hashes:\n",
    "        print(f\"Duplicate content found at: {url}, skipping.\")\n",
    "        return\n",
    "\n",
    "    # 保存该页面内容的哈希值\n",
    "    content_hashes.add(content_hash)\n",
    "\n",
    "    # 如果访问成功且内容不重复，保存这个 URL\n",
    "    successful_urls.append(url)\n",
    "    print(f\"Successfully visited: {url}\")\n",
    "\n",
    "    # 解析页面\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 查找所有的链接，使用 'a' 标签\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        # 获取链接的完整URL\n",
    "        href = link.get('href')\n",
    "        new_url = urljoin(url, href)  # 使用当前页面的 URL 作为基准\n",
    "\n",
    "        # 检查是否为相同域名的链接\n",
    "        if urlparse(new_url).netloc == urlparse(base_url).netloc:\n",
    "            # 避免爬取无效的锚点链接和邮件链接\n",
    "            if not new_url.endswith('#') and not new_url.startswith('mailto:'):\n",
    "                # 去除 URL 中的片段（#及其后内容）\n",
    "                new_url = new_url.split('#')[0]\n",
    "                # 去除 URL 中的查询参数（?及其后内容），如果不需要可以保留\n",
    "                new_url = new_url.split('?')[0]\n",
    "                # 递归爬取新的页面\n",
    "                crawl(new_url, base_url)\n",
    "\n",
    "    # 爬取结束后，稍作休眠\n",
    "    time.sleep(1)  # 避免爬虫过快爬取\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 206 unique URLs to 'successful_urls.txt'.\n"
     ]
    }
   ],
   "source": [
    "start_url = \"https://tugraph-db.readthedocs.io/zh-cn/latest/\"\n",
    "crawl(start_url, start_url)\n",
    "\n",
    "# 将成功访问的 URL 保存到文件\n",
    "with open(\"successful_urls.txt\", \"a\", encoding='utf-8') as f:\n",
    "    for url in successful_urls:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "print(f\"Successfully saved {len(successful_urls)} unique URLs to 'successful_urls.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 206 unique URLs to 'successful_urls.txt'.\n"
     ]
    }
   ],
   "source": [
    "start_url = \"https://www.oceanbase.com/docs/tugraph-doc-cn\"\n",
    "\n",
    "crawl(start_url, start_url)\n",
    "\n",
    "# 将成功访问的 URL 保存到文件\n",
    "with open(\"successful_urls.txt\", \"a\", encoding='utf-8') as f:\n",
    "    for url in successful_urls:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "print(f\"Successfully saved {len(successful_urls)} unique URLs to 'successful_urls.txt'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
